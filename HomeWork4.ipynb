{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhNosQXNG2do",
        "outputId": "2261319b-c072-4fa2-deb8-50ea3787a215"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Vocab size: 23\n",
            "Sample characters: ['\\n', ' ', 'a', 'c', 'd', 'e', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y']\n",
            "Epoch 1/10 | Train Loss: 3.1118 | Val Loss: 3.0561\n",
            "Epoch 2/10 | Train Loss: 3.0257 | Val Loss: 2.9204\n",
            "Epoch 3/10 | Train Loss: 2.8730 | Val Loss: 2.8569\n",
            "Epoch 4/10 | Train Loss: 2.8275 | Val Loss: 2.7758\n",
            "Epoch 5/10 | Train Loss: 2.7500 | Val Loss: 2.7214\n",
            "Epoch 6/10 | Train Loss: 2.7016 | Val Loss: 2.6680\n",
            "Epoch 7/10 | Train Loss: 2.6390 | Val Loss: 2.5874\n",
            "Epoch 8/10 | Train Loss: 2.5511 | Val Loss: 2.4898\n",
            "Epoch 9/10 | Train Loss: 2.4487 | Val Loss: 2.3773\n",
            "Epoch 10/10 | Train Loss: 2.3305 | Val Loss: 2.2452\n",
            "\n",
            "=== Generated (temperature = 0.7) ===\n",
            "hello thel thwexca ruel   leale eell  eoaaes ee  eio le rheel ae heoo hol gear oaeldr eel eeh ohpa x eee oouctmounay iyeid h tee lou hairer r ehre k eelel laoeh meoro  helaee hheh enlo h huheoog haee  eo ecaepraare e amlrerlwleu lhell hhuao melho gllgomima gegd eitae ee llae e ec aeeel oohlhhedh l soaig o\n",
            "\n",
            "=== Generated (temperature = 1.0) ===\n",
            "hello eae herlelthn ler aome eiodwmoarmrte  t selhelhocll  lhhhllormelli oo lhorhhie  e leuouglhir ire  vhreaigt erdewlr  curelnhg mtree  eewslllodrdhcengihhi e ei treter ne  er recateuet egr lnuogewsh unsc reo oew dime ll eall aelaiage trdhodae tee aere red lonlleeohyshrel wela aagyrrrnsr et elolholol\n",
            "lh\n",
            "\n",
            "=== Generated (temperature = 1.2) ===\n",
            "hello iaegeaewll ea aamlecalrrte elilcwnelu  oehgelia gsk\n",
            " maedp\n",
            "eot\n",
            "e ahraepm es rhe o\n",
            "looh hyepeloe d u heel ehsamtenwssa\n",
            " e etcdegp ge aerrm leeco eeo lohywma npyagueaectrrnswcanntrntepigr onetrd  edeawuyctcrtter rreel ni cwte\n",
            "g seeoca hskem wsaoygosso riuniu\n",
            "pdh\n",
            "ct dhlel alatpreiuewnacse thvvwnrornlhy\n"
          ]
        }
      ],
      "source": [
        "# Q1. Character-Level RNN Language Model\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# --------------------------\n",
        "# 1. Prepare Text Corpus\n",
        "# --------------------------\n",
        "\n",
        "# Small toy corpus (you can replace/extend this with a larger text)\n",
        "toy_text = \"\"\"\n",
        "hello hello help hello hero hello helium\n",
        "hi there how are you doing today\n",
        "this is a simple character level language model example\n",
        "we are testing recurrent neural networks on text\n",
        "\"\"\"\n",
        "\n",
        "# Optionally: If you have a text file ~50â€“200KB, you can use:\n",
        "# with open(\"my_corpus.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "#     toy_text = f.read().lower()\n",
        "\n",
        "text = toy_text.lower()\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "print(\"Vocab size:\", vocab_size)\n",
        "print(\"Sample characters:\", chars[:50])\n",
        "\n",
        "char2idx = {ch: i for i, ch in enumerate(chars)}\n",
        "idx2char = {i: ch for ch, i in char2idx.items()}\n",
        "\n",
        "def encode_text(text):\n",
        "    return [char2idx[ch] for ch in text]\n",
        "\n",
        "def decode_indices(indices):\n",
        "    return \"\".join(idx2char[i] for i in indices)\n",
        "\n",
        "encoded_text = encode_text(text)\n",
        "\n",
        "# --------------------------\n",
        "# 2. Dataset Definition\n",
        "# --------------------------\n",
        "\n",
        "class CharDataset(Dataset):\n",
        "    def __init__(self, encoded_text, seq_len=50):\n",
        "        self.data = encoded_text\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def __len__(self):\n",
        "        # last index where we can take seq_len+1 chars\n",
        "        return len(self.data) - self.seq_len - 1\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.data[idx:idx + self.seq_len]\n",
        "        y = self.data[idx + 1:idx + 1 + self.seq_len]\n",
        "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "seq_len = 50\n",
        "dataset = CharDataset(encoded_text, seq_len=seq_len)\n",
        "\n",
        "# Train/val split (e.g., 90/10)\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=batch_size)\n",
        "\n",
        "# --------------------------\n",
        "# 3. Model Definition\n",
        "# --------------------------\n",
        "\n",
        "class CharRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=64, hidden_size=128, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_size, num_layers=num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        x = self.embed(x)  # (batch, seq, embed_dim)\n",
        "        out, hidden = self.lstm(x, hidden)  # out: (batch, seq, hidden)\n",
        "        logits = self.fc(out)  # (batch, seq, vocab_size)\n",
        "        return logits, hidden\n",
        "\n",
        "model = CharRNN(vocab_size=vocab_size, embed_dim=128, hidden_size=256, num_layers=2).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# --------------------------\n",
        "# 4. Training & Validation\n",
        "# --------------------------\n",
        "\n",
        "def train_epoch(model, dataloader):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for x, y in dataloader:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits, _ = model(x)\n",
        "        # reshape for CE: (batch*seq, vocab)\n",
        "        loss = criterion(logits.view(-1, vocab_size), y.view(-1))\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)  # gradient clipping\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "def evaluate(model, dataloader):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            logits, _ = model(x)\n",
        "            loss = criterion(logits.view(-1, vocab_size), y.view(-1))\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "num_epochs = 10\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    train_loss = train_epoch(model, train_loader)\n",
        "    val_loss = evaluate(model, val_loader)\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    print(f\"Epoch {epoch}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "# --------------------------\n",
        "# 5. Text Generation\n",
        "# --------------------------\n",
        "\n",
        "def sample_next_char(probs, temperature=1.0):\n",
        "    probs = probs.cpu().numpy()\n",
        "    # temperature scaling: higher temp = more random\n",
        "    probs = np.log(probs + 1e-9) / temperature\n",
        "    probs = np.exp(probs)\n",
        "    probs = probs / np.sum(probs)\n",
        "    return np.random.choice(len(probs), p=probs)\n",
        "\n",
        "def generate_text(model, start_text=\"hello \", length=300, temperature=1.0):\n",
        "    model.eval()\n",
        "    chars_input = [char2idx.get(ch, 0) for ch in start_text.lower()]\n",
        "    input_seq = torch.tensor(chars_input, dtype=torch.long).unsqueeze(0).to(device)\n",
        "    hidden = None\n",
        "    generated = chars_input.copy()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # warm up with the start_text\n",
        "        logits, hidden = model(input_seq, hidden)\n",
        "        last_char = input_seq[:, -1]\n",
        "\n",
        "        for _ in range(length):\n",
        "            logits, hidden = model(last_char.unsqueeze(1), hidden)\n",
        "            probs = torch.softmax(logits[:, -1, :], dim=-1).squeeze(0)\n",
        "            next_idx = sample_next_char(probs, temperature=temperature)\n",
        "            generated.append(next_idx)\n",
        "            last_char = torch.tensor([next_idx], dtype=torch.long).to(device)\n",
        "\n",
        "    return decode_indices(generated)\n",
        "\n",
        "print(\"\\n=== Generated (temperature = 0.7) ===\")\n",
        "print(generate_text(model, start_text=\"hello \", length=300, temperature=0.7))\n",
        "\n",
        "print(\"\\n=== Generated (temperature = 1.0) ===\")\n",
        "print(generate_text(model, start_text=\"hello \", length=300, temperature=1.0))\n",
        "\n",
        "print(\"\\n=== Generated (temperature = 1.2) ===\")\n",
        "print(generate_text(model, start_text=\"hello \", length=300, temperature=1.2))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2. Mini Transformer Encoder for Sentences\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# --------------------------\n",
        "# 1. Toy Sentence Dataset\n",
        "# --------------------------\n",
        "\n",
        "sentences = [\n",
        "    \"i love natural language processing\",\n",
        "    \"this course is about deep learning\",\n",
        "    \"attention is all you need\",\n",
        "    \"transformers use self attention\",\n",
        "    \"rnn models process sequences\",\n",
        "    \"pytorch makes experiments easier\",\n",
        "    \"we are building a mini transformer\",\n",
        "    \"nlp tasks include classification\",\n",
        "    \"sequence to sequence models translate text\",\n",
        "    \"embedding captures word meaning\"\n",
        "]\n",
        "\n",
        "# simple whitespace tokenization\n",
        "tokenized = [s.lower().split() for s in sentences]\n",
        "vocab = sorted({w for sent in tokenized for w in sent})\n",
        "word2idx = {w: i+2 for i, w in enumerate(vocab)}  # reserve 0=PAD, 1=UNK\n",
        "word2idx[\"<pad>\"] = 0\n",
        "word2idx[\"<unk>\"] = 1\n",
        "idx2word = {i: w for w, i in word2idx.items()}\n",
        "\n",
        "vocab_size = len(word2idx)\n",
        "print(\"Vocab size:\", vocab_size)\n",
        "\n",
        "max_len = max(len(s) for s in tokenized)\n",
        "\n",
        "def encode_sentence(words, max_len):\n",
        "    ids = [word2idx.get(w, 1) for w in words]\n",
        "    if len(ids) < max_len:\n",
        "        ids += [0] * (max_len - len(ids))  # pad\n",
        "    return ids[:max_len]\n",
        "\n",
        "encoded = [encode_sentence(s, max_len) for s in tokenized]\n",
        "input_ids = torch.tensor(encoded, dtype=torch.long).to(device)  # (batch, seq_len)\n",
        "\n",
        "batch_size, seq_len = input_ids.shape\n",
        "print(\"Input shape:\", input_ids.shape)\n",
        "\n",
        "# --------------------------\n",
        "# 2. Positional Encoding\n",
        "# --------------------------\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)  # even indices\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)  # odd indices\n",
        "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len, d_model)\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return x\n",
        "\n",
        "# --------------------------\n",
        "# 3. Scaled Dot-Product Attention\n",
        "# --------------------------\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        # Q,K,V: (batch, heads, seq_len, head_dim)\n",
        "        dk = Q.size(-1)\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(dk)  # (batch, heads, seq, seq)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
        "        attn_weights = torch.softmax(scores, dim=-1)\n",
        "        output = torch.matmul(attn_weights, V)  # (batch, heads, seq, head_dim)\n",
        "        return output, attn_weights\n",
        "\n",
        "# --------------------------\n",
        "# 4. Multi-Head Attention\n",
        "# --------------------------\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model=64, num_heads=2):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.attention = ScaledDotProductAttention()\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_len, d_model = x.size()\n",
        "\n",
        "        Q = self.W_q(x)  # (batch, seq, d_model)\n",
        "        K = self.W_k(x)\n",
        "        V = self.W_v(x)\n",
        "\n",
        "        # reshape to (batch, heads, seq, head_dim)\n",
        "        def split_heads(t):\n",
        "            return t.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        Q = split_heads(Q)\n",
        "        K = split_heads(K)\n",
        "        V = split_heads(V)\n",
        "\n",
        "        attn_output, attn_weights = self.attention(Q, K, V, mask=mask)\n",
        "        # attn_output: (batch, heads, seq, head_dim)\n",
        "\n",
        "        # combine heads\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
        "        out = self.W_o(attn_output)  # (batch, seq, d_model)\n",
        "        return out, attn_weights  # attn_weights: (batch, heads, seq, seq)\n",
        "\n",
        "# --------------------------\n",
        "# 5. Feed-Forward + Encoder Layer\n",
        "# --------------------------\n",
        "\n",
        "class PositionwiseFFN(nn.Module):\n",
        "    def __init__(self, d_model=64, d_ff=128, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.dropout(self.relu(self.fc1(x))))\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model=64, num_heads=2, d_ff=128, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "        self.ffn = PositionwiseFFN(d_model=d_model, d_ff=d_ff, dropout=dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Self-attention + Add & Norm\n",
        "        attn_out, attn_weights = self.mha(x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_out))\n",
        "        # FFN + Add & Norm\n",
        "        ffn_out = self.ffn(x)\n",
        "        x = self.norm2(x + self.dropout(ffn_out))\n",
        "        return x, attn_weights\n",
        "\n",
        "# --------------------------\n",
        "# 6. Whole Encoder\n",
        "# --------------------------\n",
        "\n",
        "class MiniTransformerEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=64, num_heads=2, num_layers=1, max_len=100):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_len=max_len)\n",
        "        self.layers = nn.ModuleList([\n",
        "            EncoderLayer(d_model=d_model, num_heads=num_heads)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, input_ids, mask=None):\n",
        "        x = self.embed(input_ids)  # (batch, seq, d_model)\n",
        "        x = self.pos_encoding(x)\n",
        "        attn_weights_list = []\n",
        "        for layer in self.layers:\n",
        "            x, attn_weights = layer(x, mask)\n",
        "            attn_weights_list.append(attn_weights)\n",
        "        return x, attn_weights_list\n",
        "\n",
        "encoder = MiniTransformerEncoder(vocab_size=vocab_size, d_model=64, num_heads=2, num_layers=1, max_len=seq_len).to(device)\n",
        "\n",
        "# no training here, just a forward pass to see contextual embeddings & attention\n",
        "with torch.no_grad():\n",
        "    contextual_embeddings, attn_weights_list = encoder(input_ids)\n",
        "\n",
        "print(\"Contextual embeddings shape:\", contextual_embeddings.shape)  # (batch, seq, d_model)\n",
        "\n",
        "# --------------------------\n",
        "# 7. Show Outputs\n",
        "# --------------------------\n",
        "\n",
        "# Print input tokens and first sentence contextual vectors\n",
        "first_sentence_ids = input_ids[0].cpu().tolist()\n",
        "first_tokens = [idx2word[idx] for idx in first_sentence_ids]\n",
        "\n",
        "print(\"\\n=== Input Tokens (Sentence 1) ===\")\n",
        "print(first_tokens)\n",
        "\n",
        "print(\"\\n=== Contextual Embeddings (Sentence 1, first 5 tokens) ===\")\n",
        "print(contextual_embeddings[0, :5, :])  # first 5 tokens, all dims\n",
        "\n",
        "# Attention heatmap (head 0, layer 0, sentence 1)\n",
        "attn_weights = attn_weights_list[0]  # from first (and only) layer\n",
        "# shape: (batch, heads, seq, seq)\n",
        "attn_sentence1_head0 = attn_weights[0, 0].cpu().numpy()  # (seq, seq)\n",
        "\n",
        "print(\"\\n=== Attention Weights (Layer 1, Head 1, Sentence 1) ===\")\n",
        "print(attn_sentence1_head0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_q-OW0T9fcSE",
        "outputId": "eee3a30a-3324-45b2-df15-6fd07a2b9d30"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Vocab size: 46\n",
            "Input shape: torch.Size([10, 6])\n",
            "Contextual embeddings shape: torch.Size([10, 6, 64])\n",
            "\n",
            "=== Input Tokens (Sentence 1) ===\n",
            "['i', 'love', 'natural', 'language', 'processing', '<pad>']\n",
            "\n",
            "=== Contextual Embeddings (Sentence 1, first 5 tokens) ===\n",
            "tensor([[ 6.5521e-01,  7.3332e-01, -1.4841e+00,  9.0919e-01, -8.4982e-01,\n",
            "          1.9129e+00, -5.5165e-01,  2.7812e-01,  1.4224e-01,  7.4082e-01,\n",
            "         -1.8592e+00, -6.2796e-01, -1.0853e-01,  4.8727e-01, -2.8122e-01,\n",
            "          1.1624e+00, -3.7091e-01,  5.0361e-01, -1.0767e+00,  2.2217e-02,\n",
            "         -1.9465e-01,  1.0568e+00, -9.0730e-01,  1.4075e+00, -1.2517e+00,\n",
            "         -6.6078e-01, -1.5069e-01,  1.2949e+00, -2.5458e-01,  5.5996e-01,\n",
            "         -6.4785e-02, -5.9826e-01,  2.9142e-01,  9.4069e-01, -4.6100e-01,\n",
            "          1.5428e+00,  2.4195e-01,  6.1674e-01, -1.4491e-01, -1.5349e-02,\n",
            "         -1.2044e+00, -1.1046e+00, -1.2592e+00,  1.8414e+00,  2.9753e-02,\n",
            "          5.9955e-01, -8.7926e-01,  1.1300e+00, -1.7822e+00,  2.3558e+00,\n",
            "         -2.1220e+00,  1.7687e+00, -1.6734e+00,  4.7856e-01, -1.5640e+00,\n",
            "          7.5215e-01, -4.5823e-01,  3.6518e-01, -5.8204e-01, -1.2586e+00,\n",
            "          3.4875e-01, -7.9634e-02,  1.8024e-01,  5.3161e-01],\n",
            "        [ 2.7245e-01,  1.2882e-01,  1.2766e+00,  3.7238e-01, -1.1652e+00,\n",
            "         -7.1809e-01, -9.2506e-01,  9.1524e-01, -1.6307e+00,  4.8693e-01,\n",
            "          2.0525e-01,  1.8851e-01, -5.0183e-01,  1.0532e+00, -1.8939e+00,\n",
            "          6.4134e-03,  1.4168e-01,  8.9870e-01, -1.0015e+00,  1.9727e+00,\n",
            "         -1.6018e+00,  1.3872e-01,  2.6568e+00,  1.3361e+00, -1.1518e+00,\n",
            "          9.5146e-01, -2.8283e-01,  3.3702e-01, -3.8494e-01, -1.5077e-01,\n",
            "         -7.9097e-01,  1.2932e-01,  3.0006e-01,  1.9438e+00, -1.2345e+00,\n",
            "          3.1979e-01, -4.2691e-01,  1.1527e+00, -5.9789e-01,  9.2195e-01,\n",
            "         -1.1192e+00,  4.1262e-01, -9.7758e-01, -4.5296e-01, -6.4842e-01,\n",
            "         -5.4049e-01, -6.8531e-01, -6.5502e-01, -1.2398e+00,  9.1212e-01,\n",
            "         -1.7347e+00,  1.8011e+00, -1.3419e+00, -1.4680e-01,  1.0495e+00,\n",
            "          7.2903e-01, -9.0665e-01, -8.2461e-01, -3.5545e-01,  6.6892e-01,\n",
            "         -8.3476e-02,  3.7604e-01,  2.4694e-01,  1.8684e+00],\n",
            "        [ 1.0222e+00, -4.3611e-01, -1.5430e+00, -1.7455e+00, -4.7190e-03,\n",
            "         -1.9358e-03, -2.6633e+00, -4.1055e-01,  1.1434e+00, -2.9572e-01,\n",
            "         -1.6782e-01, -5.8700e-01, -4.1610e-01,  1.1319e+00, -6.0580e-01,\n",
            "          1.8348e+00, -1.7811e+00,  3.1029e-01, -4.3114e-01,  1.1966e+00,\n",
            "         -5.5948e-01, -5.1642e-01,  3.0077e-01,  1.5307e+00, -7.8818e-01,\n",
            "         -4.3714e-01,  8.7599e-01,  2.9236e-01, -1.0859e+00, -1.6987e-02,\n",
            "         -9.6209e-01,  6.5921e-01, -1.2026e+00,  1.1060e+00, -1.2692e-02,\n",
            "          1.6284e+00, -4.8863e-01,  1.6276e+00, -1.8540e-01,  1.1018e-02,\n",
            "         -7.8464e-01, -1.0056e+00, -1.2184e-01,  2.3373e+00,  6.5238e-01,\n",
            "          1.2643e+00, -3.7014e-01, -6.6120e-01, -1.5738e+00,  4.6442e-01,\n",
            "          5.4857e-01,  1.0110e+00, -1.5418e+00,  1.0899e+00,  2.8613e-01,\n",
            "          5.0708e-01, -1.3684e+00, -3.5023e-01,  6.9321e-01, -6.2460e-01,\n",
            "          4.4490e-01,  1.2048e+00, -1.9855e-01,  7.7071e-01],\n",
            "        [ 1.1342e+00, -1.1906e+00,  5.0754e-01, -1.3991e+00,  4.4339e-01,\n",
            "          2.9869e-01, -4.8310e-01, -3.0841e-01, -5.1559e-01, -3.7922e-01,\n",
            "          9.7433e-01, -5.8134e-01, -5.6535e-02, -8.4816e-01, -8.8678e-02,\n",
            "          6.4380e-01,  7.9446e-04,  1.4270e+00, -3.6365e-01, -1.4766e+00,\n",
            "         -1.9646e+00,  6.2033e-01, -4.0688e-01,  1.2531e+00, -5.8456e-01,\n",
            "          1.1678e+00, -1.4997e+00,  2.7462e-01,  3.1238e-01,  1.3026e+00,\n",
            "          2.5614e-01, -1.2207e+00, -1.4631e+00,  3.7390e-01, -4.8851e-02,\n",
            "          1.4885e+00, -4.4241e-01, -2.1979e-01, -1.1582e+00,  9.6952e-02,\n",
            "          2.3813e-01,  1.6299e+00, -1.7071e+00, -7.6673e-01,  7.0180e-01,\n",
            "          2.7657e-01, -1.4462e+00,  8.1923e-01, -2.8675e-01,  2.2902e+00,\n",
            "          5.6986e-01,  7.5491e-01, -7.6528e-01, -1.6542e+00,  1.4053e+00,\n",
            "         -8.5369e-02, -1.4920e+00,  6.7493e-01, -3.3371e-02, -7.9652e-02,\n",
            "          4.8200e-02,  2.3064e+00, -7.4997e-01,  1.4748e+00],\n",
            "        [ 6.8619e-01, -1.4622e+00,  6.5880e-01, -2.9457e+00,  1.1488e+00,\n",
            "         -5.9347e-01, -1.2737e+00,  1.6803e-01,  1.0437e-01, -6.0460e-01,\n",
            "          8.1384e-01, -4.6578e-01,  9.1737e-01,  3.6663e-01, -5.4155e-01,\n",
            "          9.5457e-01,  8.7201e-01, -2.2686e-02, -7.3100e-01,  5.5430e-01,\n",
            "          3.5798e-01, -1.1560e+00, -4.1230e-01,  2.3091e-01, -1.0701e+00,\n",
            "          4.7843e-01,  1.5480e+00,  1.5713e-02, -7.7499e-01, -5.8669e-01,\n",
            "          4.8821e-01,  1.9794e+00, -1.4142e+00, -6.0406e-01,  6.1429e-01,\n",
            "          1.7839e+00,  4.5733e-01,  2.5293e-01, -1.5424e+00,  3.8249e-01,\n",
            "         -1.8369e+00,  5.0055e-01, -1.8308e-01,  9.9628e-01, -1.9507e-01,\n",
            "         -5.1515e-01,  1.7685e-01,  2.1369e-01, -5.1305e-01,  3.3892e-01,\n",
            "          7.7625e-01,  5.9572e-01, -7.8485e-01,  1.1340e+00, -8.1121e-01,\n",
            "         -3.9433e-01, -4.9327e-01,  2.4391e-01, -3.5335e-01, -1.6042e+00,\n",
            "         -1.6692e+00,  9.1952e-01,  1.0167e+00,  2.8084e+00]])\n",
            "\n",
            "=== Attention Weights (Layer 1, Head 1, Sentence 1) ===\n",
            "[[0.12366376 0.3045246  0.17528674 0.10536722 0.14108256 0.15007514]\n",
            " [0.15907755 0.22997819 0.17487743 0.1691217  0.14779936 0.1191457 ]\n",
            " [0.09566073 0.28765455 0.20773168 0.14397724 0.10353599 0.16143978]\n",
            " [0.2297715  0.19728126 0.23435648 0.14139876 0.11682863 0.08036345]\n",
            " [0.13900146 0.23669219 0.20406955 0.1251348  0.12305189 0.1720501 ]\n",
            " [0.2110394  0.16408347 0.16488717 0.22311567 0.12651569 0.11035865]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q3. Implement Scaled Dot-Product Attention\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
        "    \"\"\"\n",
        "    Q, K, V: (batch, seq_len, d_k)\n",
        "    mask: (batch, seq_len, seq_len) with 0 where masked, 1 where valid (optional)\n",
        "    \"\"\"\n",
        "    dk = Q.size(-1)\n",
        "\n",
        "    # scores: (batch, seq, seq)\n",
        "    scores = torch.matmul(Q, K.transpose(-2, -1))  # unscaled\n",
        "    print(\"Raw scores (no scaling):\")\n",
        "    print(scores)\n",
        "\n",
        "    scaled_scores = scores / math.sqrt(dk)\n",
        "    print(\"\\nScaled scores (divided by sqrt(d_k)):\")\n",
        "    print(scaled_scores)\n",
        "\n",
        "    if mask is not None:\n",
        "        scaled_scores = scaled_scores.masked_fill(mask == 0, float(\"-inf\"))\n",
        "\n",
        "    attn_weights = F.softmax(scaled_scores, dim=-1)\n",
        "    print(\"\\nAttention weights after softmax:\")\n",
        "    print(attn_weights)\n",
        "\n",
        "    # output: (batch, seq, d_k)\n",
        "    output = torch.matmul(attn_weights, V)\n",
        "    print(\"\\nAttention output vectors:\")\n",
        "    print(output)\n",
        "\n",
        "    return output, attn_weights\n",
        "\n",
        "# --------------------------\n",
        "# Test with random Q, K, V\n",
        "# --------------------------\n",
        "\n",
        "batch_size = 1\n",
        "seq_len = 4\n",
        "d_k = 8\n",
        "\n",
        "torch.manual_seed(42)\n",
        "Q = torch.randn(batch_size, seq_len, d_k)\n",
        "K = torch.randn(batch_size, seq_len, d_k)\n",
        "V = torch.randn(batch_size, seq_len, d_k)\n",
        "\n",
        "print(\"Q shape:\", Q.shape)\n",
        "print(\"K shape:\", K.shape)\n",
        "print(\"V shape:\", V.shape)\n",
        "\n",
        "output, attn_weights = scaled_dot_product_attention(Q, K, V)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4oYr9t-Mf3Wk",
        "outputId": "a5c14c95-3cce-44e9-bb8d-658a68443108"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q shape: torch.Size([1, 4, 8])\n",
            "K shape: torch.Size([1, 4, 8])\n",
            "V shape: torch.Size([1, 4, 8])\n",
            "Raw scores (no scaling):\n",
            "tensor([[[-5.8108, -3.6962, -4.3623, -8.6428],\n",
            "         [-3.5381,  4.3192, -1.4130,  1.1963],\n",
            "         [-3.0255, -1.0973, -1.4376,  1.4581],\n",
            "         [-1.5793, -1.7662, -1.5380,  0.7021]]])\n",
            "\n",
            "Scaled scores (divided by sqrt(d_k)):\n",
            "tensor([[[-2.0544, -1.3068, -1.5423, -3.0557],\n",
            "         [-1.2509,  1.5271, -0.4996,  0.4229],\n",
            "         [-1.0697, -0.3879, -0.5083,  0.5155],\n",
            "         [-0.5584, -0.6244, -0.5437,  0.2482]]])\n",
            "\n",
            "Attention weights after softmax:\n",
            "tensor([[[0.1942, 0.4102, 0.3242, 0.0714],\n",
            "         [0.0408, 0.6555, 0.0864, 0.2173],\n",
            "         [0.1041, 0.2057, 0.1824, 0.5078],\n",
            "         [0.1926, 0.1803, 0.1955, 0.4316]]])\n",
            "\n",
            "Attention output vectors:\n",
            "tensor([[[ 0.2667,  0.2371, -0.0554,  0.1298,  0.3541, -0.1906, -0.6448,\n",
            "          -0.0085],\n",
            "         [ 0.1086,  0.2444, -0.2164,  0.3814,  0.0631, -0.5633, -1.1007,\n",
            "          -0.3306],\n",
            "         [ 0.4947, -0.1095, -0.5350,  0.3420, -0.6224, -0.4772,  0.3223,\n",
            "           0.2335],\n",
            "         [ 0.5705, -0.0146, -0.2775,  0.3238, -0.4680, -0.4084,  0.1981,\n",
            "           0.3296]]])\n"
          ]
        }
      ]
    }
  ]
}